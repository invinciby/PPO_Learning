# PPO Learning

> The PPO algorithm is a policy optimisation algorithm used to train policies in reinforcement learning.The PPO algorithm updates the policy by minimising the KL dispersion of the policy, thus avoiding drastic changes in the policy.The PPO algorithm updates the policy by maximising the objective function, which consists of the payoffs of the current policy and an estimate of its dominance.The PPO algorithm avoids drastic changes in the policy by truncating the objective function, thusimprove the stability of the algorithm.



> This project will use the PPO algorithm to train a policy to be able to reach a target state in a given environment.

This is my python environment:

```python
Python 3.9.20
conda create -n ppo python=3.9
conda activate ppo
pip install -r requirements.txt
```


![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*AX2vLguiKvxn-YIntIx18w.png)

```python 

1. Input: ç­–ç•¥å‚æ•°ï¼Œåˆå§‹å€¼å‡½æ•°å‚æ•°  
2. for k=0,1,2,.. do:
3.     åœ¨envä¸­é€šè¿‡è¿è¡Œç­–ç•¥å‡½æ•°Î _ké‡‡é›†è½¨è¿¹D_k
4.     è®¡ç®—æ¯æ¬¡å¥–åŠ±R_t
5.     æ ¹æ®å½“å‰çš„ä»·å€¼å‡½æ•°V_kï¼Œè®¡ç®—ä¼˜åŠ¿ä¼°è®¡å€¼A_t
6.     é€šè¿‡æœ€å¤§åŒ– PPO-Clip ç›®æ ‡æ¥æ›´æ–°æ”¿ç­–ï¼šé€šå¸¸æ˜¯é€šè¿‡Adamçš„éšæœºæ¢¯åº¦ä¸Šå‡ç®—æ³•ã€‚
7.     é€šè¿‡å‡æ–¹è¯¯å·®å›å½’æ‹Ÿåˆå€¼å‡½æ•°:é€šå¸¸æ˜¯é€šè¿‡æŸç§æ¢¯åº¦ä¸‹é™ç®—æ³•ã€‚
8. end for

```

# æ•´ä½“æµç¨‹


```mermaid
    flowchart TD
        INIT["åˆå§‹åŒ–PPOç±»"]
        HYPERPARAMS["åˆå§‹åŒ–è¶…å‚æ•°"]
        CREATE_NETS["åˆ›å»ºç­–ç•¥ç½‘ç»œå’Œå€¼å‡½æ•°ç½‘ç»œ"]
        COLLECT_DATA["æ”¶é›†æ•°æ®"]
        CALC_RTGS["è®¡ç®—å¥–åŠ±åˆ°ç›®æ ‡"]
        UPDATE_MODEL["æ›´æ–°æ¨¡å‹"]
        END(("END"))
        INIT --> HYPERPARAMS
        HYPERPARAMS --> CREATE_NETS
        CREATE_NETS --> COLLECT_DATA
        COLLECT_DATA --> CALC_RTGS
        CALC_RTGS --> UPDATE_MODEL
        UPDATE_MODEL --> COLLECT_DATA
        UPDATE_MODEL --> END
```


## ä»£ç å®ç°

1. PPOç±» - rolloutæ–¹æ³•

```mermaid
flowchart TD
    start["å¼€å§‹"]
    reset["é‡ç½®ç¯å¢ƒï¼Œè·å–åˆå§‹è§‚æµ‹"]
    get_action["è°ƒç”¨get_actionæ–¹æ³•è·å–åŠ¨ä½œå’ŒåŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡"]
    step["ä»ç¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œï¼Œè·å–æ–°çš„è§‚æµ‹ã€å¥–åŠ±å’Œæ˜¯å¦ç»“æŸ"]
    append_obs["å°†å½“å‰è§‚æµ‹æ·»åŠ åˆ°batch_obsä¸­"]
    append_action["å°†åŠ¨ä½œå’ŒåŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡æ·»åŠ åˆ°batch_actså’Œbatch_log_probsä¸­"]
    append_ep_rews["å°†å¥–åŠ±æ·»åŠ åˆ°ep_rewsä¸­"]
    check_done["æ£€æŸ¥episodeæ˜¯å¦ç»“æŸ"]
    compute_rtgs["è°ƒç”¨compute_rtgsæ–¹æ³•è®¡ç®—å¥–åŠ±çš„æŠ˜æ‰£å›æŠ¥"]
    return["è¿”å›batch_obsã€batch_actsã€batch_log_probsã€batch_rtgså’Œbatch_lens"]
    start --> reset --> get_action --> step --> append_obs --> append_action --> append_ep_rews --> check_done
    check_done -->|æ˜¯| compute_rtgs --> return
    check_done -->|å¦| get_action
    compute_rtgs --> return
```



ğŸ“• å‚è€ƒé“¾æ¥ï¼šhttps://medium.com/@eyyu/coding-ppo-from-scratch-with-pytorch-part-2-4-f9d8b8aa938a