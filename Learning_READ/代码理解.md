# 代码理解

> 需要注意的是，我们将沿着简化版PPO代码，熟悉深入PPO的原理和实现。

这里不对arguments.py代码进行讲解，因为该代码只是对一些参数进行初始化，我们直接跳过。


**从main.py开始进入**， main.py是PPO算法的入口，它主要做了以下几件事：

1. 初始化环境
    ```python
    env = gym.make(args.env_name)
    ```
2. 定义神经网络

    这里使用最基本的nn.Linear作为神经网络，定义了两个网络，一个用于actor，一个用于critic。详见network.py，这里不作介绍。

3. 初始化模型
    ```python
    # main.py
    actor_model=args.actor_model, 
    critic_model=args.critic_model
    //
    model.actor.load_state_dict(torch.load(actor_model))
    model.critic.load_state_dict(torch.load(critic_model))

    # PPO_v1.py
    self.actor = policy_class(self.obs_dim, self.act_dim)
    self.critic = policy_class(self.obs_dim, 1)
    ```

4. 训练模型
    1. 指定训练的总时间步数total_timesteps，即一共要玩多少次游戏
    2. 对于一次游戏`learn(self, total_timesteps)`，主要做了以下几件事：

        a. 收集游戏的数据`rollout(self)`
            
        重置环境
            
        For：

            保存obs_0，即初始状态

            获取动作和概率`get_action(self, obs)`

            执行动作，获取下一个状态和奖励`env.step(acts)`

            更新obs_1，即下一个状态

        保存数据【episode和奖励】

        计算rewards-to-go回报`compute_rtgs(self, batch_rews)`

        b.更新学习率，协方差矩阵

        c. 计算值函数估计，优势函数

        d. 多次更新策略和值函数
            


       

