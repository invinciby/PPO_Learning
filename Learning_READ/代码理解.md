# 代码理解

> 需要注意的是，我们将沿着简化版PPO代码，熟悉深入PPO的原理和实现。

这里不对arguments.py代码进行讲解，因为该代码只是对一些参数进行初始化，我们直接跳过。


**从main.py开始进入**， main.py是PPO算法的入口，它主要做了以下几件事：

1. 初始化环境
```python
env = gym.make(args.env_name)
```
2. 初始化模型
```python
# main.py
actor_model=args.actor_model, critic_model=args.critic_model

model.actor.load_state_dict(torch.load(actor_model))
model.critic.load_state_dict(torch.load(critic_model))

# PPO_v1.py
self.actor = policy_class(self.obs_dim, self.act_dim)
self.critic = policy_class(self.obs_dim, 1)
```

3. 训练模型

