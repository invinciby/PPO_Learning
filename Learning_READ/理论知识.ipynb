{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始\n",
    "\n",
    "首先我们需要了解RL的概念，RL主要是Agent在环境中不断的交互，通过试错的方式学习到最优的**策略**，从而实现最大化奖励的目标。\n",
    "在交互的每一步，Agent会感知来自环境的奖励信号，这个数字告诉它当前环境状态的好坏，Agent会根据这个信号来更新自己的策略。\n",
    "\n",
    "需要清楚的是，强化学习的数学基础就算马尔可夫决策MDP，一个MDP 通常由状态空间、动作空间、状态转移矩阵、奖励函数以及折扣因子等组成。\n",
    "\n",
    "首先，我们定义一下相关的术语：\n",
    "\n",
    "- 状态S：是对世界状态的完整描述，没有任何关于世界的信息隐藏在状态中。\n",
    "- 观察o：是对状态的部分描述，可能会省略一些信息。\n",
    "\n",
    "> 这里可以理解为一个小球是s，而o就是我们用光将其投影在墙面上得到的影子。因此o是不全面的认知，而s是全面的认知。\n",
    "\n",
    "在RL中，我们通常使用向量，矩阵或高阶张量来表示状态和观察。例如，视觉观察可以有像素值RGB矩阵表示，而机器人的状态可能由关节角度和速度向量表示。\n",
    "\n",
    "- 动作空间A：不同环境允许不同的动作$a_t$，给定环境中所有有效动作的集合为动作空间\n",
    "- 策略：策略是Agent在给定状态下选择动作的规则。策略可以表示为函数，该函数接受状态作为输入并输出动作。策略可以是有确定性的，也可以是随机的。确定性策略总是选择相同的动作，而随机策略可能会选择不同的动作，即使给定相同的输入状态。\n",
    "    \n",
    "    一般情况下，策略函数是一个条件概率密度函数 $\\pi (a|s) = P(A=a|S=s)$\n",
    "\n",
    "    - 确定情况下，通常用 $\\mu$ : $a_t = \\mu(s_t)$ \n",
    "    - 随机情况下，通常用 $\\pi$ : $a_t \\sim \\pi(·|s_t)$\n",
    "\n",
    "    这里举个例子，在*超级玛丽*这个游戏中，他只能上左右三个动作，假设策略函数输出动作的概率为：\n",
    "        $$\\pi (左|s)=0.2$$\n",
    "        $$\\pi (右|s)=0.1$$\n",
    "        $$\\pi (上|s)=0.7$$\n",
    "    那么Agent就会做一个随机抽样：有0.2的概率向左走，有0.1的概率向右走，有0.7的概率向上跳。\n",
    "    \n",
    "在深度 RL 中，我们处理参数化策略：其输出是可计算函数的策略，这些函数依赖于一组参数（例如神经网络的权重和偏差），我们可以通过一些优化算法来调整这些参数以改变行为。\n",
    "通常用 或 \\phi 来 \\theta 表示此类策略的参数，然后将其作为下标写入策略符号上以突出显示连接：\n",
    "    $$a_t = \\mu_\\phi (s_t)$$ \n",
    "    $$a_t \\sim_\\phi \\pi(·|s_t)$$\n",
    "\n",
    "- 奖励r：奖励是环境对Agent采取动作的反馈，通常是一个标量值。奖励函数通常表示为 $R(s_t, a_t)$，其中 $s_t$ 是在时间步 $t$ 时Agent的状态，$a_t$ 是Agent在时间步 $t$ 时采取的动作。奖励函数可以是有确定性的，也可以是随机的。确定性奖励函数总是输出相同的奖励，而随机奖励函数可能会输出不同的奖励，即使给定相同的输入状态和动作。**而奖励函数通常是由我们来定义。**\n",
    "\n",
    "- 状态转移：是指当前状态s变成新的状态s’。给定当前的状态s，Agent执行动作a后，环境给出下一时刻状态s’。而这个新的状态s’是环境通过状态转移函数计算出来的，它能够将(s,a)映射到s’。\n",
    "- 状态转移函数：是可以确定的，也可以是随机的。**确定性状态转移函数**比如下棋，当前状态s，和玩家执行动作a，就可以确定新的状态s’，而**随机状态转移函数**比如超级玛丽中，玛丽如果向上跳，其位置就上去了，这个是确定的，但是其敌人的位置可以往左也可能往由，因此难以确定下一状态。\n",
    "    - 随机状态转移函数通常记作$p(s'|s,a)=P(S'=s'|S=s,A=a)$，也是一个条件概率密度函数。意思是如果观测到当前状态 s 以及动作a，那么p函数就能输出状态变成s'的概率。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
